{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# call OpenAI API with model name and prompt\n",
    "def call_openai_api(prompt, model_name='text-davinci-003', max_token=100, stop=None, n=1, temperature=0.1):\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_version = \"2022-12-01\"\n",
    "    \n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY') # replace this with your OpenAI API Key\n",
    "    openai.api_base = os.getenv(\"OPENAI_API_BASE\") # replace this with your OpenAI API Endpoint\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=model_name,\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_token,\n",
    "        stop=stop,\n",
    "        n=n,\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought\n",
    "\n",
    "Experiment results demonstrate Zero-shot-CoT using single prompt template, significantly outperform zero-shot LLM performance on diverse benchmark reasoning tasks.  Without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002).\n",
    "\n",
    "Source: [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.\n"
     ]
    }
   ],
   "source": [
    "# This prompt gets wrong answer\n",
    "\n",
    "PROMPT_ZERO_SHOT = \"\"\"Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
    "and half of the golf balls are blue. How many blue golf balls are\n",
    "there?\n",
    "A: The answer (arabic numerals) is\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_ZERO_SHOT, model_name='text-davinci-003', temperature=0, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 8.\n"
     ]
    }
   ],
   "source": [
    "# Still wrong answer with few-shot learning\n",
    "\n",
    "PROMPT_FEW_SHOT = \"\"\"Q: Roger has 5 tennis balss. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Roger have now?\n",
    "A: The answer is 11.\n",
    "\n",
    "Q: A juggler can juggle 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\n",
    "A:\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT, model_name='text-davinci-003', temperature=0, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First, the juggler can juggle 16 balls.\n",
      "\n",
      "Second, half of the balls are golf balls. So, there are 8 golf balls.\n",
      "\n",
      "Third, half of the golf balls are blue. So, there are 4 blue golf balls.\n"
     ]
    }
   ],
   "source": [
    "# With CoT, the answer is correct\n",
    "\n",
    "PROMPT_ZERO_SHOT_CoT = \"\"\"Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
    "and half of the golf balls are blue. How many blue golf balls are\n",
    "there?\n",
    "A: Letâ€™s think step by step.\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_ZERO_SHOT_CoT, model_name='text-davinci-003', temperature=0, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half of 16 balls is 8. Half of 8 golf balls is 4. Therefore, there are 4 blue golf balls.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROMPT_FEW_SHOT_CoT = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis\n",
    "balls. Each can has 3 tennis balls. How many tennis balls does\n",
    "he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6\n",
    "tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
    "and half of the golf balls are blue. How many blue golf balls are\n",
    "there?\n",
    "A:\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT_CoT, model_name='text-davinci-003', temperature=0, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More research on CoT prompt engineering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program-aided Language Models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning. They sold 93 loaves in the morning and 39 loaves in the afternoon. 6 loaves were returned. 200 - (93 + 39 + 6) = 62. The answer is 62 loaves of bread left.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_FEW_SHOT_CoT = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of\n",
    "tennis balls. Each can has 3 tennis balls. How many\n",
    "tennis balls does he have now?\n",
    "A: Roger started with 5 tennis balls. 2 cans of 3 tennis\n",
    "balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: The bakers at the Beverly Hills Bakery baked 200\n",
    "loaves of bread on Monday morning. They sold 93 loaves\n",
    "in the morning and 39 loaves in the afternoon. A grocery\n",
    "store returned 6 unsold loaves. How many loaves of\n",
    "bread did they have left?\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT_CoT, model_name='text-davinci-003', temperature=0, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning.\n",
      "  baked_loaves = 200\n",
      "They sold 93 loaves in the morning and 39 loaves in the afternoon.\n",
      "  sold_loaves = 93 + 39\n",
      "A grocery store returned 6 unsold loaves.\n",
      "  returned_loaves = 6\n",
      "The answer is\n",
      "answer = baked_loaves - sold_loaves - returned_loaves\n"
     ]
    }
   ],
   "source": [
    "PROMPT_FEW_SHOT_PA = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of\n",
    "tennis balls. Each can has 3 tennis balls. How many\n",
    "tennis balls does he have now?\n",
    "A: Roger started with 5 tennis balls.\n",
    "  tennis_balls = 5\n",
    "2 cans of 3 tennis balls each is\n",
    "  bought_balls = 2 * 3 \n",
    "The answer is\n",
    "answer = tennis_balls + bought_balls\n",
    "\n",
    "Q: The bakers at the Beverly Hills Bakery baked 200\n",
    "loaves of bread on Monday morning. They sold 93 loaves\n",
    "in the morning and 39 loaves in the afternoon. A grocery\n",
    "store returned 6 unsold loaves. How many loaves of bread\n",
    "did they have left?\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT_PA, model_name='text-davinci-003', temperature=0, max_token=400)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Reasoning\n",
    "\n",
    "Paper: [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
    "\n",
    "\n",
    "\n",
    "Provide knowledge, turn knowledge question into reasoning. In general, more knowledge, better result.\n",
    "\n",
    "3 Contributing factors:\n",
    "\n",
    "(i) the quality of knowledge, \n",
    "\n",
    "(ii) the quantity of knowledge where the performance improves with more knowledge statements, and \n",
    "\n",
    "(iii) the strategy for integrating knowledge during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"The player with the lowest score wins.\n",
    "Is this true or false: Part of golf is trying to get a higher point total than others.\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT, model_name='text-davinci-003', n=1, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An easel typically has three legs.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"A tripod is a kind of easel\n",
    "How many legs does an easel have?\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT, model_name='text-davinci-003', n=1, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out follow 2 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, the objective of golf is not to get a higher point total than others. The objective of golf is to play a set of holes in the least number of strokes. The total number of strokes is used to determine the winner of the game, not the total number of points.\n"
     ]
    }
   ],
   "source": [
    "# High confidence answer\n",
    "PROMPT = \"\"\"Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "\n",
    "Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
    "\n",
    "Explain and Answer: \n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT, model_name='text-davinci-003', n=1, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, part of golf is trying to get a higher point total than others. The goal of the game is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game, so players are competing to get the highest point total in order to win.\n"
     ]
    }
   ],
   "source": [
    "# Low confidence answer\n",
    "PROMPT = \"\"\"Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "\n",
    "Knowledge: Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
    "\n",
    "Explain and Answer:\n",
    " \n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT, model_name='text-davinci-003', n=1, max_token=100)\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
